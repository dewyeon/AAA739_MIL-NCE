{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAA739 Lecture 7: Self-supervised Representation Learning\n",
    "## End-to-End Learning of Visual Representations from Uncurated Instructional Videos\n",
    "- CVPR 2020 Oral\n",
    "- paper link: https://arxiv.org/abs/1912.06430\n",
    "- official code link (github): https://github.com/antoine77340/MIL-NCE_HowTo100M\n",
    "- presenter: 2022020918 Juyeon Ko"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Getting preliminary word2vec and HowTo100M data for training\n",
    "You will first need to download the word2vec matrix and dictionary and unzip the file in the same directory as the code, in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://www.rocq.inria.fr/cluster-willow/amiech/word2vec.zip\n",
    "unzip word2vec.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will need to download the preprocessed HowTo100M captions and unzip the csv files somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wget https://www.rocq.inria.fr/cluster-willow/amiech/howto100m/howto100m_captions.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally the preprocessed HowTo100M videos (12Tb in total) can be downloaded by filling this Google form: https://forms.gle/hztrfnFQUJWBtiki8. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Training MIL-NCE on HowTo100M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.optim\n",
    "import torch.multiprocessing as mp\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "\n",
    "import s3dg\n",
    "from args import get_args\n",
    "from video_loader import HT100M_DataLoader\n",
    "from loss import MILNCELoss\n",
    "\n",
    "from metrics import compute_metrics\n",
    "from youcook_loader import Youcook_DataLoader\n",
    "from utils import AllGather\n",
    "from utils import get_cosine_schedule_with_warmup\n",
    "\n",
    "allgather = AllGather.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### video_loader.py\n",
    "\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import ffmpeg\n",
    "import time\n",
    "import re\n",
    "\n",
    "class HT100M_DataLoader(Dataset):\n",
    "    \"\"\"HowTo100M Video-Text loader.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            csv,\n",
    "            video_root='',\n",
    "            caption_root='',\n",
    "            min_time=4.0,\n",
    "            fps=16,\n",
    "            num_frames=16,\n",
    "            size=224,\n",
    "            crop_only=False,\n",
    "            center_crop=True,\n",
    "            benchmark=False,\n",
    "            token_to_word_path='data/dict.npy',\n",
    "            max_words=20,\n",
    "            num_candidates=1,\n",
    "            random_left_right_flip=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        assert isinstance(size, int)\n",
    "        self.csv = pd.read_csv(os.path.join(os.path.dirname(__file__), csv))\n",
    "        self.video_root = video_root\n",
    "        self.caption_root = caption_root\n",
    "        self.min_time = min_time\n",
    "        self.size = size\n",
    "        self.num_frames = num_frames\n",
    "        self.fps = fps\n",
    "        self.num_sec = self.num_frames / float(self.fps)\n",
    "        self.crop_only = crop_only\n",
    "        self.center_crop = center_crop\n",
    "        self.benchmark = benchmark\n",
    "        self.max_words = max_words\n",
    "        token_to_word = np.load(os.path.join(os.path.dirname(__file__), token_to_word_path))\n",
    "        self.word_to_token = {}\n",
    "        for i, t in enumerate(token_to_word):\n",
    "            self.word_to_token[t] = i + 1\n",
    "        self.num_candidates = num_candidates\n",
    "        self.random_flip = random_left_right_flip\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def _get_video(self, video_path, start, end):\n",
    "        start_seek = random.randint(start, int(max(start, end - self.num_sec)))\n",
    "        cmd = (\n",
    "            ffmpeg\n",
    "            .input(video_path, ss=start_seek, t=self.num_sec + 0.1)\n",
    "            .filter('fps', fps=self.fps)\n",
    "        )\n",
    "        if self.center_crop:\n",
    "            aw, ah = 0.5, 0.5\n",
    "        else:\n",
    "            aw, ah = random.uniform(0, 1), random.uniform(0, 1)\n",
    "        if self.crop_only:\n",
    "            cmd = (\n",
    "                cmd.crop('(iw - {})*{}'.format(self.size, aw),\n",
    "                         '(ih - {})*{}'.format(self.size, ah),\n",
    "                         str(self.size), str(self.size))\n",
    "            )\n",
    "        else:\n",
    "            cmd = (\n",
    "                cmd.crop('(iw - min(iw,ih))*{}'.format(aw),\n",
    "                         '(ih - min(iw,ih))*{}'.format(ah),\n",
    "                         'min(iw,ih)',\n",
    "                         'min(iw,ih)')\n",
    "                .filter('scale', self.size, self.size)\n",
    "            )\n",
    "        if self.random_flip and random.uniform(0, 1) > 0.5:\n",
    "            cmd = cmd.hflip()\n",
    "        out, _ = (\n",
    "            cmd.output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "            .run(capture_stdout=True, quiet=True)\n",
    "        )\n",
    "        video = np.frombuffer(out, np.uint8).reshape([-1, self.size, self.size, 3])\n",
    "        video = th.from_numpy(video)\n",
    "        video = video.permute(3, 0, 1, 2)\n",
    "        if video.shape[1] < self.num_frames:\n",
    "            zeros = th.zeros((3, self.num_frames - video.shape[1], self.size, self.size), dtype=th.uint8)\n",
    "            video = th.cat((video, zeros), axis=1)\n",
    "        return video[:, :self.num_frames]\n",
    "\n",
    "    def _split_text(self, sentence):\n",
    "        w = re.findall(r\"[\\w']+\", str(sentence))\n",
    "        return w\n",
    "\n",
    "    def _words_to_token(self, words):\n",
    "        words = [self.word_to_token[word] for word in words if word in self.word_to_token]\n",
    "        if words:\n",
    "            we = self._zero_pad_tensor_token(th.LongTensor(words), self.max_words)\n",
    "            return we\n",
    "        else:\n",
    "            return th.zeros(self.max_words, dtype=th.long)\n",
    "\n",
    "    def _zero_pad_tensor_token(self, tensor, size):\n",
    "        if len(tensor) >= size:\n",
    "            return tensor[:size]\n",
    "        else:\n",
    "            zero = th.zeros(size - len(tensor)).long()\n",
    "            return th.cat((tensor, zero), dim=0)\n",
    "\n",
    "    def words_to_ids(self, x):\n",
    "        return self._words_to_token(self._split_text(x))\n",
    "\n",
    "    def _find_nearest_candidates(self, caption, ind):\n",
    "        start, end = ind, ind\n",
    "        diff = caption['end'][end] - caption['start'][start]\n",
    "        n_candidate = 1\n",
    "        while n_candidate < self.num_candidates:\n",
    "           if start == 0:\n",
    "               return 0\n",
    "           elif end == len(caption) - 1:\n",
    "               return start - (self.num_candidates - n_candidate)\n",
    "           elif caption['end'][end] - caption['start'][start - 1] < caption['end'][end + 1] - caption['start'][start]:\n",
    "               start -= 1\n",
    "           else:\n",
    "               end += 1\n",
    "           n_candidate += 1\n",
    "        return start\n",
    "\n",
    "    def _get_text(self, caption):\n",
    "        cap = pd.read_csv(caption)\n",
    "        ind = random.randint(0, len(cap) - 1)\n",
    "        if self.num_candidates == 1:\n",
    "            words = self.words_to_ids(cap['text'].values[ind])\n",
    "        else:\n",
    "            words = th.zeros(self.num_candidates, self.max_words, dtype=th.long)\n",
    "            cap_start = self._find_nearest_candidates(cap, ind)\n",
    "            for i in range(self.num_candidates):\n",
    "                words[i] = self.words_to_ids(cap['text'].values[max(0, min(len(cap['text']) - 1, cap_start + i))])\n",
    "        start, end = cap['start'].values[ind], cap['end'].values[ind]\n",
    "        #TODO: May need to be improved for edge cases. \n",
    "        if end - start < self.min_time:\n",
    "            diff = self.min_time - end + start\n",
    "            start = max(0, start - diff / 2)\n",
    "            end = start + self.min_time \n",
    "        return words, int(start), int(end) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.csv['video_path'][idx]\n",
    "        video_id = video_file.split('.')[0]\n",
    "        video_path = os.path.join(self.video_root, video_file)\n",
    "        text, start, end = self._get_text(os.path.join(self.caption_root, video_id + '.csv'))\n",
    "        video = self._get_video(video_path, start, end)\n",
    "        return {'video': video, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loss.py\n",
    "\n",
    "class MILNCELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MILNCELoss, self).__init__()\n",
    "\n",
    "    def forward(self, video_embd, text_embd):\n",
    "        x = torch.matmul(video_embd, text_embd.t())\n",
    "        x = x.view(video_embd.shape[0], video_embd.shape[0], -1)\n",
    "        nominator = x * torch.eye(x.shape[0])[:,:,None].cuda()\n",
    "        nominator = nominator.sum(dim=1)\n",
    "        nominator = torch.logsumexp(nominator, dim=1)\n",
    "        denominator = torch.cat((x, x.permute(1,0,2)), dim=1).view(x.shape[0], -1)\n",
    "        denominator = torch.logsumexp(denominator, dim=1)\n",
    "        return torch.mean(denominator - nominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, ngpus_per_node, args):\n",
    "    args.gpu = gpu\n",
    "    if args.distributed:\n",
    "        if args.multiprocessing_distributed:\n",
    "            args.rank = args.rank * ngpus_per_node + gpu\n",
    "        dist.init_process_group(\n",
    "            backend=args.dist_backend,\n",
    "            init_method=args.dist_url,\n",
    "            world_size=args.world_size,\n",
    "            rank=args.rank,\n",
    "        )\n",
    "    # create model\n",
    "    model = s3dg.S3D(\n",
    "        args.num_class, space_to_depth=False, word2vec_path=args.word2vec_path, init=args.weight_init,\n",
    "    )\n",
    "\n",
    "    if args.pretrain_cnn_path:\n",
    "        net_data = torch.load(args.pretrain_cnn_path)\n",
    "        model.load_state_dict(net_data)\n",
    "    if args.distributed:\n",
    "        if args.gpu is not None:\n",
    "            torch.cuda.set_device(args.gpu)\n",
    "            model.cuda(args.gpu)\n",
    "            args.batch_size = int(args.batch_size / ngpus_per_node)\n",
    "            args.batch_size_val = int(args.batch_size_val / ngpus_per_node)\n",
    "            args.num_thread_reader = int(args.num_thread_reader / ngpus_per_node)\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        else:\n",
    "            model.cuda()\n",
    "            model = torch.nn.parallel.DistributedDataParallel(model)\n",
    "    elif args.gpu is not None:\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        model = model.cuda(args.gpu)\n",
    "    else:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "    # Data loading code\n",
    "    train_dataset = HT100M_DataLoader(\n",
    "        csv=args.train_csv,\n",
    "        video_root=args.video_path,\n",
    "        caption_root=args.caption_root,\n",
    "        min_time=args.min_time,\n",
    "        fps=args.fps,\n",
    "        num_frames=args.num_frames,\n",
    "        size=args.video_size,\n",
    "        crop_only=args.crop_only,\n",
    "        center_crop=args.centercrop,\n",
    "        random_left_right_flip=args.random_flip,\n",
    "        num_candidates=args.num_candidates,\n",
    "    )\n",
    "    # Test data loading code\n",
    "    test_dataset = Youcook_DataLoader(\n",
    "        data=os.path.join(os.path.dirname(__file__), 'csv/validation_youcook.csv'),\n",
    "        num_clip=args.num_windows_test,\n",
    "        video_root=args.eval_video_root,\n",
    "        fps=args.fps,\n",
    "        num_frames=args.num_frames,\n",
    "        size=args.video_size,\n",
    "        crop_only=False,\n",
    "        center_crop=True,\n",
    "    )\n",
    "    if args.distributed:\n",
    "        train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset)\n",
    "        test_sampler = torch.utils.data.distributed.DistributedSampler(test_dataset)\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        test_sampler = None\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=(train_sampler is None),\n",
    "        drop_last=True,\n",
    "        num_workers=args.num_thread_reader,\n",
    "        pin_memory=args.pin_memory,\n",
    "        sampler=train_sampler,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=args.batch_size_val,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "        num_workers=args.num_thread_reader,\n",
    "        sampler=test_sampler,\n",
    "    )\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    criterion = MILNCELoss()\n",
    "\n",
    "    if args.optimizer == 'adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), args.lr)\n",
    "    elif args.optimizer == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), args.lr, momentum=args.momemtum)\n",
    "\n",
    "    scheduler = get_cosine_schedule_with_warmup(optimizer, args.warmup_steps, len(train_loader) * args.epochs)\n",
    "    checkpoint_dir = os.path.join(os.path.dirname(__file__), 'checkpoint', args.checkpoint_dir)\n",
    "    if args.checkpoint_dir != '' and not(os.path.isdir(checkpoint_dir)) and args.rank == 0:\n",
    "        os.mkdir(checkpoint_dir)\n",
    "    # optionally resume from a checkpoint\n",
    "    if args.resume:\n",
    "        checkpoint_path = get_last_checkpoint(checkpoint_dir)\n",
    "        if checkpoint_path:\n",
    "            log(\"=> loading checkpoint '{}'\".format(checkpoint_path), args)\n",
    "            checkpoint = torch.load(checkpoint_path)\n",
    "            args.start_epoch = checkpoint[\"epoch\"]\n",
    "            model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "            optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "            scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "            log(\"=> loaded checkpoint '{}' (epoch {})\".format(checkpoint_path, checkpoint[\"epoch\"]), args)\n",
    "        else:\n",
    "            log(\"=> no checkpoint found at '{}'\".format(args.resume), args)\n",
    "\n",
    "    if args.cudnn_benchmark:\n",
    "        cudnn.benchmark = True\n",
    "    total_batch_size = args.world_size * args.batch_size \n",
    "    log(\n",
    "        \"Starting training loop for rank: {}, total batch size: {}\".format(\n",
    "            args.rank, total_batch_size\n",
    "        ), args\n",
    "    )\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        if args.distributed:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "        if epoch % max(1, total_batch_size // 512) == 0 and args.evaluate:\n",
    "            evaluate(test_loader, model, epoch, args, 'YouCook2')\n",
    "        # train for one epoch\n",
    "        train(train_loader, model, criterion, optimizer, scheduler, epoch, train_dataset, args)\n",
    "        if args.rank == 0:\n",
    "            save_checkpoint(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"scheduler\": scheduler.state_dict(),\n",
    "                }, checkpoint_dir, epoch + 1\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainOneBatch(model, opt, scheduler, data, loss_fun, args):\n",
    "    video = data[\"video\"].float().cuda(args.gpu, non_blocking=args.pin_memory)\n",
    "    text = data[\"text\"].cuda(args.gpu, non_blocking=args.pin_memory)\n",
    "    text = text.view(-1, text.shape[-1])\n",
    "    video = video / 255.0\n",
    "    opt.zero_grad()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        video_embd, text_embd = model(video, text)\n",
    "        if args.distributed:\n",
    "            video_embd = allgather(video_embd, args)\n",
    "            text_embd = allgather(text_embd, args)\n",
    "        loss = loss_fun(video_embd, text_embd)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    scheduler.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, scheduler, epoch, dataset, args):\n",
    "    running_loss = 0.0\n",
    "    s = time.time()\n",
    "    for i_batch, sample_batch in enumerate(train_loader):\n",
    "        s_step = time.time()\n",
    "        batch_loss = TrainOneBatch(model, optimizer, scheduler, sample_batch, criterion, args)\n",
    "        d_step = time.time() - s_step\n",
    "        running_loss += batch_loss\n",
    "        if (i_batch + 1) % args.n_display == 0 and args.verbose and args.rank == 0:\n",
    "            d = time.time() - s\n",
    "            log(\n",
    "                \"Epoch %d, Elapsed Time: %.3f, Epoch status: %.4f, Training loss: %.4f, Learning rate: %.6f\"\n",
    "                % (\n",
    "                    epoch + 1,\n",
    "                    d,\n",
    "                    args.batch_size * args.world_size * float(i_batch) / len(dataset),\n",
    "                    running_loss / args.n_display,\n",
    "                    optimizer.param_groups[0]['lr'],\n",
    "                ), args\n",
    "            )\n",
    "            running_loss = 0.0\n",
    "            s = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_loader, model, epoch, args, dataset_name):\n",
    "    all_txt_embd = []\n",
    "    all_video_embd = []\n",
    "    model.eval()\n",
    "    if args.rank == 0:  \n",
    "        log('Evaluating on {}'.format(dataset_name), args)\n",
    "    with torch.no_grad():\n",
    "        for i_batch, data in enumerate(test_loader):\n",
    "            text = data['text'].cuda()\n",
    "            video = data['video'].float().cuda()\n",
    "            video = video / 255.0\n",
    "            video = video.view(-1, video.shape[2], video.shape[3], video.shape[4], video.shape[5])\n",
    "            video_embd, text_embd = model(video, text)\n",
    "            video_embd = video_embd.view(text_embd.shape[0], args.num_windows_test, text_embd.shape[1])\n",
    "            video_embd = video_embd.mean(dim=1)\n",
    "            video_embd = allgather(video_embd, args)\n",
    "            text_embd = allgather(text_embd, args)\n",
    "            if args.rank == 0:\n",
    "                text_embd = text_embd.cpu().numpy()\n",
    "                video_embd = video_embd.cpu().numpy()\n",
    "                all_txt_embd.append(text_embd)\n",
    "                all_video_embd.append(video_embd)\n",
    "    model.train()\n",
    "    if args.rank == 0:\n",
    "        all_txt_embd = np.concatenate(all_txt_embd, axis=0)\n",
    "        all_video_embd = np.concatenate(all_video_embd, axis=0)\n",
    "        metrics = compute_metrics(np.dot(all_txt_embd, all_video_embd.T))\n",
    "        log('Epoch {} results: {}'.format(epoch, metrics), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, checkpoint_dir, epoch, n_ckpt=10):\n",
    "    torch.save(state, os.path.join(checkpoint_dir, \"epoch{:0>4d}.pth.tar\".format(epoch)))\n",
    "    if epoch - n_ckpt >= 0:\n",
    "        oldest_ckpt = os.path.join(checkpoint_dir, \"epoch{:0>4d}.pth.tar\".format(epoch - n_ckpt)) \n",
    "        if os.path.isfile(oldest_ckpt):\n",
    "            os.remove(oldest_ckpt)\n",
    "\n",
    "def get_last_checkpoint(checkpoint_dir):\n",
    "    all_ckpt = glob.glob(os.path.join(checkpoint_dir, 'epoch*.pth.tar'))\n",
    "    if all_ckpt:\n",
    "        all_ckpt = sorted(all_ckpt)\n",
    "        return all_ckpt[-1]\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def log(output, args):\n",
    "    with open(os.path.join(os.path.dirname(__file__), 'log' , args.checkpoint_dir + '.txt'), \"a\") as f:\n",
    "        f.write(output + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_distributed():\n",
    "    args = get_args()\n",
    "    if args.verbose:\n",
    "        print(args)\n",
    "    assert args.eval_video_root != '' or not(args.evaluate)\n",
    "    assert args.video_path != ''\n",
    "    assert args.caption_root != ''\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "\n",
    "    if args.world_size == -1 and \"SLURM_NPROCS\" in os.environ:\n",
    "        args.world_size = int(os.environ[\"SLURM_NPROCS\"])\n",
    "        args.rank = int(os.environ[\"SLURM_PROCID\"])\n",
    "        jobid = os.environ[\"SLURM_JOBID\"]\n",
    "        hostfile = \"dist_url.\" + jobid + \".txt\"\n",
    "        args.dist_url = \"file://{}.{}\".format(os.path.realpath(args.dist_file), jobid)\n",
    "        print(\n",
    "            \"dist-url:{} at PROCID {} / {}\".format(\n",
    "                args.dist_url, args.rank, args.world_size\n",
    "            )\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    " \n",
    "    args.distributed = args.world_size > 1 or args.multiprocessing_distributed\n",
    "    ngpus_per_node = torch.cuda.device_count()\n",
    "    if args.multiprocessing_distributed:\n",
    "        args.world_size = ngpus_per_node * args.world_size\n",
    "        mp.spawn(main_worker, nprocs=ngpus_per_node, args=(ngpus_per_node, args))\n",
    "    else:\n",
    "        main_worker(args.gpu, ngpus_per_node, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python main_distributed.py --n_display=1 \\\n",
    "       --multiprocessing-distributed --batch_size=256 \\\n",
    "       --num_thread_reader=40 --cudnn_benchmark=1 --pin_memory \\\n",
    "       --checkpoint_dir=milnce --num_candidates=4 --resume --lr=0.001 \\\n",
    "       --warmup_steps=10000 --epochs=300 --caption_root=path_to_howto_csv --video_path=path_to_howto_videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Evaluation on MSR-VTT Retrieval Task\n",
    "#### Download the MSR-VTT videos\n",
    "A mirror link of the MSR-VTT testing videos can be found at: https://www.mediafire.com/folder/h14iarbs62e7p/shared\n",
    "#### Evaluation\n",
    "This evaluation will run the zero-shot text-video retrieval on the MSR-VTT subset of the test set used in [1]. You will need to replace the_path_to_the_checkpoint by your model checkpoint path and path_to_the_msrvtt_videos to the root folder containing the downloaded MSR-VTT testing videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation code for MSR-VTT Retreival\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "from metrics import compute_metrics, print_computed_metrics\n",
    "from args import get_args\n",
    "from msrvtt_loader import MSRVTT_DataLoader\n",
    "import s3dg\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### msrvtt_loader.py\n",
    "\n",
    "import torch as th\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import ffmpeg\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "class MSRVTT_DataLoader(Dataset):\n",
    "    \"\"\"MSRVTT Video-Text loader.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            data,\n",
    "            video_root='',\n",
    "            num_clip=4,\n",
    "            fps=16,\n",
    "            num_frames=32,\n",
    "            size=224,\n",
    "            crop_only=False,\n",
    "            center_crop=True,\n",
    "            token_to_word_path='data/dict.npy',\n",
    "            max_words=30,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        \"\"\"\n",
    "        assert isinstance(size, int)\n",
    "        self.data = pd.read_csv(data)\n",
    "        self.video_root = video_root\n",
    "        self.size = size\n",
    "        self.num_frames = num_frames\n",
    "        self.fps = fps\n",
    "        self.num_clip = num_clip\n",
    "        self.num_sec = self.num_frames / float(self.fps)\n",
    "        self.crop_only = crop_only\n",
    "        self.center_crop = center_crop\n",
    "        self.max_words = max_words\n",
    "        self.word_to_token = {}\n",
    "        token_to_word = np.load(os.path.join(os.path.dirname(__file__), token_to_word_path))\n",
    "        for i, t in enumerate(token_to_word):\n",
    "            self.word_to_token[t] = i + 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def _get_video(self, video_path, start, end, num_clip):\n",
    "        video = th.zeros(num_clip, 3, self.num_frames, self.size, self.size)\n",
    "        start_ind = np.linspace(start, max(start, end-self.num_sec - 0.4), num_clip) \n",
    "        for i, s in enumerate(start_ind):\n",
    "            video[i] = self._get_video_start(video_path, s) \n",
    "        return video\n",
    "\n",
    "    def _get_video_start(self, video_path, start):\n",
    "        start_seek = start\n",
    "        cmd = (\n",
    "            ffmpeg\n",
    "            .input(video_path, ss=start_seek, t=self.num_sec + 0.1)\n",
    "            .filter('fps', fps=self.fps)\n",
    "        )\n",
    "        if self.center_crop:\n",
    "            aw, ah = 0.5, 0.5\n",
    "        else:\n",
    "            aw, ah = random.uniform(0, 1), random.uniform(0, 1)\n",
    "        if self.crop_only:\n",
    "            cmd = (\n",
    "                cmd.crop('(iw - {})*{}'.format(self.size, aw),\n",
    "                         '(ih - {})*{}'.format(self.size, ah),\n",
    "                         str(self.size), str(self.size))\n",
    "            )\n",
    "        else:\n",
    "            cmd = (\n",
    "                cmd.crop('(iw - min(iw,ih))*{}'.format(aw),\n",
    "                         '(ih - min(iw,ih))*{}'.format(ah),\n",
    "                         'min(iw,ih)',\n",
    "                         'min(iw,ih)')\n",
    "                .filter('scale', self.size, self.size)\n",
    "            )\n",
    "        out, _ = (\n",
    "            cmd.output('pipe:', format='rawvideo', pix_fmt='rgb24')\n",
    "            .run(capture_stdout=True, quiet=True)\n",
    "        )\n",
    "        video = np.frombuffer(out, np.uint8).reshape([-1, self.size, self.size, 3])\n",
    "        video = th.from_numpy(video)\n",
    "        video = video.permute(3, 0, 1, 2)\n",
    "        if video.shape[1] < self.num_frames:\n",
    "            zeros = th.zeros((3, self.num_frames - video.shape[1], self.size, self.size), dtype=th.uint8)\n",
    "            video = th.cat((video, zeros), axis=1)\n",
    "        return video[:, :self.num_frames]\n",
    "\n",
    "    def _split_text(self, sentence):\n",
    "        w = re.findall(r\"[\\w']+\", str(sentence))\n",
    "        return w\n",
    "\n",
    "    def _words_to_token(self, words):\n",
    "        words = [self.word_to_token[word] for word in words if word in self.word_to_token]\n",
    "        if words:\n",
    "            we = self._zero_pad_tensor_token(th.LongTensor(words), self.max_words)\n",
    "            return we\n",
    "        else:\n",
    "            return th.zeros(self.max_words).long()\n",
    "\n",
    "    def _zero_pad_tensor_token(self, tensor, size):\n",
    "        if len(tensor) >= size:\n",
    "            return tensor[:size]\n",
    "        else:\n",
    "            zero = th.zeros(size - len(tensor)).long()\n",
    "            return th.cat((tensor, zero), dim=0)\n",
    "\n",
    "    def words_to_ids(self, x):\n",
    "        return self._words_to_token(self._split_text(x))\n",
    "\n",
    "    def _get_duration(self, video_path):\n",
    "        probe = ffmpeg.probe(video_path)\n",
    "        return probe['format']['duration']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_id = self.data['video_id'].values[idx]\n",
    "        cap = self.data['sentence'].values[idx]\n",
    "        video_path = os.path.join(self.video_root, video_id + '.mp4')\n",
    "        duration = self._get_duration(video_path)\n",
    "        text = self.words_to_ids(cap)\n",
    "        video = self._get_video(video_path, 0, float(duration), self.num_clip)\n",
    "        return {'video': video, 'text': text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(train_loader, model, args):\n",
    "    all_txt_embd = []\n",
    "    all_video_embd = []\n",
    "    with torch.no_grad():\n",
    "        for i_batch, data in enumerate(tqdm(train_loader)):\n",
    "            text = data['text'].cuda()\n",
    "            video = data['video'].float().cuda()\n",
    "            video = video / 255.0\n",
    "            video = video.view(-1, video.shape[2], video.shape[3], video.shape[4], video.shape[5])\n",
    "            video_embd, text_embd = model(video, text)\n",
    "            text_embd  = text_embd.cpu().numpy()\n",
    "            video_embd = video_embd.view(text_embd.shape[0], args.num_windows_test, text_embd.shape[1])\n",
    "            video_embd = video_embd.mean(dim=1)\n",
    "            video_embd  = video_embd.cpu().numpy()\n",
    "            all_txt_embd.append(text_embd)\n",
    "            all_video_embd.append(video_embd)\n",
    "    all_txt_embd = np.concatenate(all_txt_embd, axis=0)\n",
    "    all_video_embd = np.concatenate(all_video_embd, axis=0)\n",
    "    metrics = compute_metrics(np.dot(all_txt_embd, all_video_embd.T))\n",
    "    print_computed_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### eval_msrvtt.py \n",
    "\n",
    "def eval_msrvtt():\n",
    "    args = get_args()\n",
    "    assert args.eval_video_root != ''\n",
    "    checkpoint_path = args.pretrain_cnn_path\n",
    "    print(\"=> loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    if \"state_dict\" in checkpoint:\n",
    "        model = s3dg.S3D(\n",
    "            args.num_class, space_to_depth=False, word2vec_path=args.word2vec_path)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    else: # load pre-trained model from https://github.com/antoine77340/S3D_HowTo100M\n",
    "        model = s3dg.S3D(\n",
    "            args.num_class, space_to_depth=True, word2vec_path=args.word2vec_path)\n",
    "        model = torch.nn.DataParallel(model)\n",
    "        checkpoint_module = {'module.' + k:v for k,v in checkpoint.items()}\n",
    "        model.load_state_dict(checkpoint_module)\n",
    "    model.eval()\n",
    "    model.cuda()\n",
    "   \n",
    "    # Data loading code\n",
    "    dataset = MSRVTT_DataLoader(\n",
    "        data=os.path.join(os.path.dirname(__file__), 'csv/msrvtt_test.csv'),\n",
    "        num_clip=args.num_windows_test,\n",
    "        video_root=args.eval_video_root,\n",
    "        fps=args.fps,\n",
    "        num_frames=args.num_frames,\n",
    "        size=args.video_size,\n",
    "        crop_only=False,\n",
    "        center_crop=True,\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=args.num_thread_reader,\n",
    "    )\n",
    "    # train for one epoch\n",
    "    evaluate(dataloader, model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`python eval_msrvtt.py --batch_size=16 --num_thread_reader=20 --num_windows_test=10 \\\n",
    "    --eval_video_root=path_to_the_videos --pretrain_cnn_path=the_path_to_the_checkpoint`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
